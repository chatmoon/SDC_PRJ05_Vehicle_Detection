{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writeup | Vehicle tracking\n",
    "---\n",
    "[![Udacity](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Note for the reviewer:** the video frame stops/freezes three times for 0.5 second. It is because I had to cut the video in three pieces. My laptop is not powerful enough to process the all video at once.  Please accept my apologies for this inconvenient. `May I ask you to mail me the feedback review by email (chatmoon@gmail.com) please? Thank you, #mo` _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract â€” This notebook is the writeup of the Vehicle Tracking project**  as part of the SELF-DRIVING CAR nanodegree program. We apply a combinaison of machine learning and computer vision techniques using `OpenCV` and `scikit-learn` functions to the task of vehicle tracking in a video from a front-facing camera on a car. To that end, we use\n",
    "Support Vector Machine (SVM) Classifier, Histogram of Gradients (HOG), Spatial binary and Color Histogram techniques.   \n",
    "\n",
    "The project is broken down into the following steps:\n",
    "* Extract histogram of oriented gradients (HOG) feature vector\n",
    "* Train support vector machine (SVM) classifier\n",
    "* Normalize the features and randomize a selection for training and testing\n",
    "* Detect vehicles frame by frame\n",
    "* Reject outliers creating a heat map of recurring detections frame by frame \n",
    "* Track vehicle and estimate a bounding box for vehicles detected \n",
    "\n",
    "\n",
    "Here I will consider [**the rubric points**](https://review.udacity.com/#!/rubrics/513/view) individually and describe how I addressed each point in my implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Histogram of Oriented Gradients (HOG)\n",
    "---\n",
    "#### 1. Extract histogram of oriented gradients (HOG) feature vector from the training images\n",
    "\n",
    "*The code for this part is contained in lines 30 through 151 of the file called `main.py`. It is split into several functions, i.e. `get_hog_features()`, `bin_spatial()`, `color_hist()`, in lines 60 through 151. There are all combined in the function called `extract_features()`.*  \n",
    "\n",
    "I used a dataset that has two types of image and label: **8,793** `RGB` images with car and **8,968** images without car. I started by reading in all the vehicle and non-vehicle images. The code listing all the images is contained in lines 30 through 56 of the file `main.py`, in the `list_files()` and `list_all_images()` methods.\n",
    "\n",
    "Here is an example of five of each of the `vehicle` and `non-vehicle` classes:\n",
    "![fig.0](https://raw.githubusercontent.com/chatmoon/SDC_PRJ05_Vehicle_Detection/master/output_images/_sample%20of%20cars%20and%20notcars_5%20items.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_hog_features()` extracts the HOG feature vector:   \n",
    "\n",
    "```python\n",
    "# -- lines 60 through 77 of the file main.py --\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, vis=False, feature_vec=True):    \n",
    "    if vis==True: # call with two outputs if vis==True\n",
    "        features, hog_image = hog(img,orientations=orient,\n",
    "                                  pixels_per_cell = (pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block = (cell_per_block, cell_per_block),\n",
    "                                  transform_sqrt  = True,\n",
    "                                  visualize = vis, feature_vector = feature_vec )\n",
    "        return features, hog_image\n",
    "    \n",
    "    else: # otherwise call with one output\n",
    "        features = hog(img, orientations=orient,\n",
    "                                  pixels_per_cell = (pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block = (cell_per_block, cell_per_block),\n",
    "                                  transform_sqrt  = True,\n",
    "                                  visualize = vis, feature_vector = feature_vec )\n",
    "        return features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then explored different color spaces and different `skimage.hog()` parameters (`orientations`, `pixels_per_cell`, and `cells_per_block`).\n",
    "\n",
    "I grabbed random images from each of the two classes and displayed them to get a feel for what the `skimage.hog()` output looks like. Here is an example using the `RGB` color space and HOG parameters of `orientations=6`, `pixels_per_cell=(8, 8)` and `cells_per_block=(2, 2)`:\n",
    "![fig.1](https://raw.githubusercontent.com/chatmoon/SDC_PRJ05_Vehicle_Detection/master/output_images/_hog_image_orient-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `bin_spatial()` extracts the binned color features:   \n",
    "```python\n",
    "# -- lines 80 through 86 of the file main.py --\n",
    "def bin_spatial(img, size=(32, 32)):\n",
    "    # create the feature vector\n",
    "    color1 = cv2.resize(img[:, :, 0], size).ravel()\n",
    "    color2 = cv2.resize(img[:, :, 1], size).ravel()\n",
    "    color3 = cv2.resize(img[:, :, 2], size).ravel()\n",
    "    return np.hstack((color1, color2, color3))\n",
    "```\n",
    "\n",
    "The hog features and the binned color features are combined. They complement each other in the information they capture about the vehicle. Subsequently, I will also combine the histograms of color features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `color_hist()` extracts the histogram color features:\n",
    "```python\n",
    "# -- lines 90 through 98 of the file main.py --\n",
    "def color_hist(img, nbins=32):\n",
    "    # compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram( img[:, :, 0], bins=nbins )\n",
    "    channel2_hist = np.histogram( img[:, :, 1], bins=nbins )\n",
    "    channel3_hist = np.histogram( img[:, :, 2], bins=nbins )\n",
    "    # concatenate the histograms into a single feautre vector\n",
    "    hist_features  = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    # return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These feautres are all combined into a main feature vector by the `extract_features()` method:\n",
    "```python\n",
    "# -- lines 102 through 151 of the file main.py --\n",
    "def extract_features(imgs, color_space='RGB', spatial_size=(32, 32), hist_bins=32, orient=9, pix_per_cell=8, \n",
    "                     cell_per_block=2, hog_channel=0, spatial_feat=True, hist_feat=True, hog_feat=True):    \n",
    "    features = []  # create a list to append feature vectors to\n",
    "    for file in imgs: # iterate through the list of images\n",
    "        file_features = []        \n",
    "        image = mpimg.imread(file) # read in each one by one\n",
    "        feature_image = convert_color(image, conv=color_space) # apply color conversation if other than 'RGB'\n",
    "        if spatial_feat == True:\n",
    "            spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "            file_features.append(spatial_features)\n",
    "        if hist_feat == True:            \n",
    "            hist_features = color_hist(feature_image, nbins=hist_bins) # apply color_hist()\n",
    "            file_features.append(hist_features)\n",
    "        if hog_feat == True:\n",
    "            if hog_channel == 'ALL': # call get_hog_feature() with vis=False, feature_vec=True\n",
    "                hog_features = []\n",
    "                for channel in range(feature_image.shape[2]):\n",
    "                    hog_features.append(get_hog_features(feature_image[:,:,channel], orient,\n",
    "                                        pix_per_cell, cell_per_block, vis=False, feature_vec=True))\n",
    "                hog_features = np.ravel(hog_features)\n",
    "            else:\n",
    "                hog_features = get_hog_features(feature_image[:,:,hog_channel], orient,\n",
    "                                                pix_per_cell, cell_per_block, vis=False, feature_vec=True)           \n",
    "            file_features.append(hog_features) # append the new feature vector to the features list\n",
    "        features.append(np.concatenate(file_features))\n",
    "    # return list of feature vectors\n",
    "    return features\n",
    "```\n",
    "\n",
    "All combined features make the vehicle dection algorithm more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Explain how you settled on your final choice of HOG parameters.\n",
    "\n",
    "I tried various combinations of parameters to optimize at best both the accuracy and the sum of the feature computation time and the SVC training time. For this purpose, I created the `exp_hog_parameters()` function, lines 19 through 95 of the `hog_parameters.py` file. I applied a color transform and append binned color features, as well as histograms of color, to the HOG feature vector.  \n",
    "\n",
    "_**Note:** I did these experiments with the whole dataset instead of a smaller one. It was a mistake. I would be able to shorter the time spent in this part of the project._\n",
    "\n",
    "Here are a few results of these various combinations of parameters:\n",
    "\n",
    "> a. _various color space_:   \n",
    "\n",
    "| color_space | cell_per_block| hist_bins | hist_feat | hog_channel | orientation  | pix_per_cell | spatial_feat | spatial_size | accuracy | feature vec. length | seconds to compute features | seconds to train SVC |\n",
    "| :-------------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- |\n",
    "| HLS | 2 | 64 | True | ALL | 8 | 8 | True | (32, 32) | 0.990 | 7968 | 12.58 | 10.48| \n",
    "| HSV | 2 | 64 | True | ALL | 8 | 8 | True | (32, 32) | 0.990 | 7968 | 12.14 | 9.18| \n",
    "| LUV | 2 | 64 | True | ALL | 8 | 8 | True | (32, 32) | 0.985 | 7968 | 11.77 | 0.69| \n",
    "| RGB | 2 | 64 | True | ALL | 8 | 8 | True | (32, 32) | 0.995 | 7968 | 12.50 | 8.08| \n",
    "| RGB | 2 | 64 | True | ALL | 8 | 8 | True | (32, 32) | 0.970 | 7968 | 11.66 | 8.32| \n",
    "| YCrCb | 2 | 64 | True | ALL | 8 | 8 | True | (32, 32) | 0.995 | 7968 | 67.31 | 8.90| \n",
    "| YUV | 2 | 64 | True | ALL | 8 | 8 | True | (32, 32) | 0.990 | 7968 | 12.12 | 0.78|   \n",
    "\n",
    "> b. _YCrCb with various orientation_:   \n",
    "\n",
    "| orientation | cell_per_block | color_space | hist_bins | hist_feat | hog_channel | pix_per_cell | spatial_feat | spatial_size | accuracy | feature vec. length | seconds to compute features | seconds to train SVC |\n",
    "| :-------------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- |\n",
    "| 6 | 2 | YCrCb | 64 | True | ALL | 8 | True | (32, 32) | 1.000 | 7968 | 11.76 | 8.16| \n",
    "| 8 | 2 | YCrCb | 64 | True | ALL | 8 | True | (32, 32) | 0.995 | 7968 | 67.31 | 8.90| \n",
    "| 9 | 2 | YCrCb | 64 | True | ALL | 8 | True | (32, 32) | 0.995 | 7968 | 11.75 | 1.28| \n",
    "| 10 | 2 | YCrCb | 64 | True | ALL | 8 | True | (32, 32) | 0.975 | 7968 | 11.82 | 8.14| \n",
    "| 12 | 2 | YCrCb | 64 | True | ALL | 8 | True | (32, 32) | 0.995 | 7968 | 11.87 | 8.71| \n",
    "| 14 | 2 | YCrCb | 64 | True | ALL | 8 | True | (32, 32) | 0.995 | 7968 | 12.11 | 9.89| \n",
    "| 16 | 2 | YCrCb | 64 | True | ALL | 8 | True | (32, 32) | 0.995 | 7968 | 11.72 | 8.63|    \n",
    "\n",
    "> c. _RGB with various orientation_:   \n",
    "\n",
    "| orientation | cell_per_block | color_space | hist_bins | hist_feat | hog_channel | pix_per_cell | spatial_feat | spatial_size | accuracy | feature vec. length | seconds to compute features | seconds to train SVC |\n",
    "| :-------------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- |\n",
    "| 6 | 2 | RGB | 64 | True | ALL | 8 | True | (32, 32) | 0.985 | 7968 | 11.96 | 0.91| \n",
    "| 8 | 2 | RGB | 64 | True | ALL | 8 | True | (32, 32) | 0.995 | 7968 | 12.50 | 8.08| \n",
    "| 9 | 2 | RGB | 64 | True | ALL | 8 | True | (32, 32) | 0.980 | 7968 | 12.01 | 8.33| \n",
    "| 10 | 2 | RGB | 64 | True | ALL | 8 | True | (32, 32) | 0.980 | 7968 | 11.82 | 9.28| \n",
    "| 12 | 2 | RGB | 64 | True | ALL | 8 | True | (32, 32) | 0.990 | 7968 | 11.90 | 8.99| \n",
    "| 14 | 2 | RGB | 64 | True | ALL | 8 | True | (32, 32) | 0.985 | 7968 | 11.82 | 9.97| \n",
    "| 16 | 2 | RGB | 64 | True | ALL | 8 | True | (32, 32) | 1.000 | 7968 | 11.85 | 8.64| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After several experiments, I finally chose to go with the following values that give both a good accuracy and a satisfactory sum of the feature computation time and the SVC training time for the tracker:   \n",
    "\n",
    "| Parameters      |  Values  |\n",
    "| :-------------- | :------- |\n",
    "| cell_per_block  | 2       |\n",
    "| color_space     | YCrCb   |\n",
    "| hist_bins     | 64   |\n",
    "| hist_feat     | True   |\n",
    "| hog_channel     | ALL   |\n",
    "| orientation     | 9       |\n",
    "| pix_per_cell    | 8       |\n",
    "| spatial_feat     | True   |\n",
    "| spatial_size     | (32, 32) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Describe how you trained a classifier using your selected HOG features and color features\n",
    "\n",
    "Before training the classifier, the combinaison of feature vectors need to be normalized. To do this, I used `sklearn.preprocessing.StandardScaler()` class to standardize features by removing the mean and scaling to unit variance. It is done to prevent machine learning estimators to behave badly.\n",
    "\n",
    "```python\n",
    "# -- lines 335 through 338 of the file main.py in the function classifier() --\n",
    "X_scaler = StandardScaler().fit(X) # fit a per_column scaler\n",
    "scaled_X = X_scaler.transform(X)   # apply the scaler to X (doc: \"Perform standardization by centering and scaling\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I shuffled and split randomly the dataset of **17,761** `RGB` images in total (8,793 vehicle and 8,968 non-vehicle images) in two sets: 80% as training data and 20% as testing images. I used the `sklearn.model_selection.train_test_split()` function for this.\n",
    "\n",
    "```python\n",
    "# -- lines 343 through 345 of the file main.py in the function classifier() --\n",
    "# split up data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "# NB: the shuffle parameter is boolean and the default is always TRUE (see doc)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I trained a linear SVM using `sklearn.svm.LinearSVC()` function:\n",
    "\n",
    "```python\n",
    "# -- lines 351 through 355 of the file main.py in the function classifier() --\n",
    "svc = LinearSVC() # use a linear SVC\n",
    "svc.fit(X_train, y_train) \n",
    "```\n",
    "\n",
    "And the trained classifier is saved on the hard drive in `data\\pickled_object` as `svc.pkl file`:\n",
    "```python\n",
    "# -- lines 359 through 362 of the file main.py in the function classifier() --\n",
    "# serialize/store svc:\n",
    "joblib.dump(svc, args.pickled + 'svc.pkl')\n",
    "```\n",
    "\n",
    "I measured the mean accuracy on the test data and labels using `sklearn.svm.LinearSVC.score()` method:\n",
    "```python\n",
    "# -- lines 351 through 355 of the file main.py in the function classifier() --\n",
    "# check the score of the SVC\n",
    "accuracy = round(svc.score(X_test, y_test), 4)\n",
    "print('Test Accuracy of SVC = ', accuracy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean accuracy is about 99.5%:   \n",
    "\n",
    "| orientation | cell_per_block | color_space | hist_bins | hist_feat | hog_channel | pix_per_cell | spatial_feat | spatial_size | accuracy | feature vec. length | seconds to compute features | seconds to train SVC |\n",
    "| :-------------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- |\n",
    "| 9 | 2 | YCrCb | 64 | True | ALL | 8 | True | (32, 32) | 0.995 | 7968 | 11.75 | 1.28| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Sliding Window Search\n",
    "---\n",
    "#### 1. Describe how you implemented a sliding window search. How did you decide what scales to search and how much to overlap windows?\n",
    "\n",
    "The algorithm is looking for cars only in the lower half of the frame ([for now](https://goo.gl/yVx4gZ)). The scan is done with a overlapping of 0.5. I implemented a minimum and maximum scale `(0.75, 1.75)` and two additional and intermdiate scales to scan as well: `(1., 1.5)` in the function `multiscale_bboxes()`:\n",
    "```python\n",
    "# -- lines 473 through 478 of the file main.py --\n",
    "def multiscale_bboxes(args, var, image):\n",
    "    bboxes = []\n",
    "    for var['scale'] in var['scales']:\n",
    "        bboxes_scaled = find_cars(args, var, image)\n",
    "        bboxes        = bboxes + bboxes_scaled\n",
    "    return bboxes\n",
    "```\n",
    "\n",
    "Here is the visualization of the scanning of the window:\n",
    "![fig.2](https://goo.gl/485qR3)\n",
    "\n",
    "The algorithm is far from perfect. Nevertheless, the cars are dectected properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole process has been implemented in the function `find_cars()`. Unfortunately, it is slow. It could have been optimized by redesigning the scanning method as it was suggested during the lesson.\n",
    "\n",
    "```python\n",
    "# -- lines 383 through 470 of the file main.py --\n",
    "def find_cars(args, var, image):\n",
    "    # load the trained classifier\n",
    "    X_scaler,_,svc = classifier(args, var, to_print=False) \n",
    "    bboxes = []\n",
    "    img    = image.copy().astype(np.float32) / 255\n",
    "    img_tosearch = img[var['y_start_stop'][0]:var['y_start_stop'][1], :, :]\n",
    "    ctrans_tosearch = convert_color(img_tosearch, conv=var['color_space'])\n",
    "    if var['scale'] != 1:\n",
    "        imshape = ctrans_tosearch.shape\n",
    "        ctrans_tosearch = cv2.resize(ctrans_tosearch,\n",
    "                          (np.int(imshape[1] / var['scale']), np.int(imshape[0] / var['scale'])))\n",
    "    if var['hog_channel'] == 'ALL':\n",
    "        ch1 = ctrans_tosearch[:, :, 0]\n",
    "        ch2 = ctrans_tosearch[:, :, 1]\n",
    "        ch3 = ctrans_tosearch[:, :, 2]\n",
    "    else:\n",
    "        ch1 = ctrans_tosearch[:, :, var['hog_channel']]\n",
    "    # Define blocks and steps as above\n",
    "    nxblocks = (ch1.shape[1] // var['pix_per_cell']) - var['cell_per_block'] + 1\n",
    "    nyblocks = (ch1.shape[0] // var['pix_per_cell']) - var['cell_per_block'] + 1\n",
    "    nfeat_per_block = var['orient'] * var['cell_per_block'] ** 2\n",
    "    # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\n",
    "    window = 64\n",
    "    nblocks_per_window = (window // var['pix_per_cell']) - var['cell_per_block'] + 1\n",
    "    cells_per_step = 2  # Instead of overlap, define how many cells to step\n",
    "    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step + 1\n",
    "    nysteps = (nyblocks - nblocks_per_window) // cells_per_step + 1\n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    if var['hog_channel'] == 'ALL':\n",
    "        hog1 = get_hog_features(ch1, var['orient'], var['pix_per_cell'],\n",
    "                                var['cell_per_block'], feature_vec=False)\n",
    "        hog2 = get_hog_features(ch2, var['orient'], var['pix_per_cell'],\n",
    "                                var['cell_per_block'], feature_vec=False)\n",
    "        hog3 = get_hog_features(ch3, var['orient'], var['pix_per_cell'],\n",
    "                                var['cell_per_block'], feature_vec=False)\n",
    "    else:\n",
    "        hog1 = get_hog_features(ch1, var['orient'], var['pix_per_cell'],\n",
    "                                var['cell_per_block'], feature_vec=False)\n",
    "\n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            ypos, xpos = yb * cells_per_step, xb * cells_per_step\n",
    "            # Extract HOG for this patch\n",
    "            if var['hog_channel'] == 'ALL': \n",
    "                hog_feat1 = hog1[ypos:ypos + nblocks_per_window,\n",
    "                                 xpos:xpos + nblocks_per_window].ravel()\n",
    "                hog_feat2 = hog2[ypos:ypos + nblocks_per_window,\n",
    "                                 xpos:xpos + nblocks_per_window].ravel()\n",
    "                hog_feat3 = hog3[ypos:ypos + nblocks_per_window,\n",
    "                                 xpos:xpos + nblocks_per_window].ravel()\n",
    "                hog_features = np.hstack((hog_feat1, hog_feat2, hog_feat3))\n",
    "            else:\n",
    "                hog_features = hog1[ypos:ypos + nblocks_per_window,\n",
    "                                    xpos:xpos + nblocks_per_window].ravel()\n",
    "            ytop, xleft = ypos * var['pix_per_cell'], xpos * var['pix_per_cell']\n",
    "            # Extract the image patch\n",
    "            subimg = cv2.resize(ctrans_tosearch[ytop:ytop + window, xleft:xleft + window], (64, 64))\n",
    "            # Get color features\n",
    "            spatial_features = bin_spatial(subimg, size=var['spatial_size'])\n",
    "            hist_features = color_hist(subimg, nbins=var['hist_bins'])\n",
    "            # var['scale'] features and make a prediction\n",
    "            test_features = X_scaler.transform(\n",
    "                np.hstack((spatial_features, hist_features, hog_features)).reshape(1, -1))\n",
    "            # test_features = X_scaler.transform(np.hstack((shape_feat, hist_feat)).reshape(1, -1))\n",
    "            test_prediction = svc.predict(test_features)\n",
    "            if test_prediction == 1:\n",
    "                xbox_left = np.int(xleft * var['scale'])\n",
    "                ytop_draw = np.int(ytop * var['scale'])\n",
    "                win_draw = np.int(window * var['scale'])\n",
    "                bboxes.append(((xbox_left, ytop_draw+var['y_start_stop'][0]),\n",
    "                               xbox_left+win_draw,ytop_draw+win_draw+var['y_start_stop'][0])))\n",
    "    return bboxes \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Show some examples of test images to demonstrate how your pipeline is working.  What did you do to optimize the performance of your classifier?\n",
    "\n",
    "Ultimately I searched on **four** scales using `YCrCb` 3-channel HOG features plus spatially binned color and histograms of color in the feature vector, which provided a nice result.  Here are some example images:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig](https://goo.gl/jHmutU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Video Implementation\n",
    "---\n",
    "#### 1. The final video output\n",
    "Here's the link to my video result: www.youtube.com/watch?v=vTU3D7OdSFY   \n",
    "\n",
    "[![video.1](https://img.youtube.com/vi/vTU3D7OdSFY/0.jpg)](https://youtu.be/vTU3D7OdSFY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Describe how you implemented some kind of filter for false positives and some method for combining overlapping bounding boxes.\n",
    "*The code for this part is contained in lines 482 through 536 of the file called `main.py`. It is split into several functions, i.e. `add_heat()`, `apply_threshold()`, `draw_labeled_bboxes()`, in lines 482 through 511. There are all combined in the function called `process_image()`.*   \n",
    "\n",
    "I recorded the positions of positive detections in each frame of the video.\n",
    "```python\n",
    "# -- line 524 of the file main.py in the function process_image() --\n",
    "    bboxes = multiscale_bboxes(args, var, image)\n",
    "```\n",
    "From the positive detections I created a heatmap to filter out false positives by increasing artificially the pixel values of all bounding boxes.\n",
    "```python\n",
    "# -- lines 351 through 355 of the file main.py in the function add_heat() --\n",
    "def add_heat(heatmap, bbox_list):   \n",
    "    for box in bbox_list:  # Iterate through list of bboxes\n",
    "        # Add += 1 for all pixels inside each bbox = ((x1, y1), (x2, y2))\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "    return heatmap\n",
    "```\n",
    "The more bounding boxes are overlapping, the higher the values are in those regions of the frame, meaning the classifier has recognized several times vehicle(s) in this region during the scan. And then thresholded that map to identify vehicle positions.\n",
    "```python\n",
    "# -- lines 493 through 496 of the file main.py in the function apply_threshold() --\n",
    "def apply_threshold(heatmap, threshold):\n",
    "    heatmap[heatmap <= threshold] = 0 # zeros out pixels below the threshold\n",
    "    return heatmap\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recorded the positions of positive detections in each frame of the video. From the positive detections I created a heatmap and then thresholded that map to identify vehicle positions. I then used `scipy.ndimage.measurements.label()` to identify individual blobs in the heatmap. I then assumed each blob corresponded to a vehicle. I constructed bounding boxes to cover the area of each blob detected.\n",
    "\n",
    "Here's an example result showing the heatmap from a series of frames of video, the result of `scipy.ndimage.measurements.label()` and the bounding boxes then overlaid on the last frame of video:   \n",
    "![fig.0333](https://goo.gl/SDtMhC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the output of `scipy.ndimage.measurements.label()` on the integrated heatmap:\n",
    "![fig.6](https://scontent-ams3-1.xx.fbcdn.net/v/t1.0-9/37695282_10155936612373772_5643635104807911424_n.jpg?_nc_cat=0&oh=25ff7502ff734944edcc0beeb8d0ec3d&oe=5C1426E7) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heapmap technique is very useful to reduce false positives. To Strengthen the algorithm, I reused an idea learnt in a previous project, i.e. I stored any new heat values and used an average of the last N entries. This last implementation cleaned up the remaining false positives.   \n",
    " \n",
    "Here the resulting bounding boxes are drawn onto the last frame in the series:\n",
    "![fig.0408](https://goo.gl/2oJGzX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Discussion\n",
    "\n",
    "#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?\n",
    "\n",
    "I enjoyed learning about these techniques. I wish I had more free time to dig more about the sliding window search.   \n",
    "\n",
    "We use a classifier as a base of the solution. Then I expect that the pipeline is as good as the quality and the diversity of the dataset. Certainly it will be difficult to generalize the use of this pipeline to any new environment. \n",
    "\n",
    "Futhermore, it is slow and resource demanding. By optimizing the sliding window search algorithm, I am sure that it would have a significant impact on the pipeline performance.   \n",
    "\n",
    "I would love to get the same kind of video but with billboards on the sides during a motor show in town :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "ANNEXE\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the list of functions created and used to complete this project:\n",
    "\n",
    "| Function  |  Line   |  Description                                            |\n",
    "| :------------------------------| :-----: | :------------------------------------------------------ |\n",
    "| `module\\main.py\\list_files()`  | 30-47 | list all files in the folder and save the list in output_images folder |\n",
    "| `module\\main.py\\list_all_images()`  | 50-56 | list all `vehicle` and `non-vehicle` images and save the lists in `output_images` folder  |\n",
    "| `module\\main.py\\get_hog_features()`  | 60-76 | return HOG features and visualization |\n",
    "| `module\\main.py\\bin_spatial()`  | 80-85 | compute binned color features |\n",
    "| `module\\main.py\\color_hist()`  | 89-97 | compute color histogram features |\n",
    "| `module\\main.py\\extract_features()`  | 101-150 | extract features from a list of images |\n",
    "| `module\\main.py\\slide_window()`  | 154-188 | take an image, start and stop positions in both x and y, window size (x and y dimensions), and overlap fraction (for both x and y) |\n",
    "| `module\\main.py\\draw_boxes()`  | 192-200 | draw bounding boxes |\n",
    "| `module\\main.py\\single_img_features()`  | 203-239 | extract features from a single image window (for a single image) |\n",
    "| `module\\main.py\\search_windows()`  | 243-264 | pass an image and the list of windows to be searched |\n",
    "| `module\\main.py\\visualize()`  | 280-295 | plot multiple images |\n",
    "| `module\\main.py\\classifier()`  | 298-365 | return a trained Linear Support Vector Classification (svc) classifier and save it in `data\\pickled_object` folder |\n",
    "| `module\\main.py\\convert_color()`  | 368-380 | convert one image from one RGB color-space to another color-space (can be RGB HSV LUV HLS YUV YCrCb) |\n",
    "| `module\\main.py\\find_cars()`  | 384-472 | extract features using hog sub-sampling and make predictions |\n",
    "| `module\\main.py\\multiscale_bboxes()`  | 475-480 | extract features using hog sub-sampling and make predictions for various scales (0.75,1.,1.5, 1.75) |\n",
    "| `module\\main.py\\add_heat()`  | 484-492 | add \"heat\" to a map for a list of bounding boxes |\n",
    "| `module\\main.py\\apply_threshold()`  | 495-498 | threshold the map to reject areas affected |\n",
    "| `module\\main.py\\draw_labeled_bboxes()`  | 502-513 | return the image labeled with bboxes |\n",
    "| `module\\main.py\\process_image()`  | 529-553 | track vehicle in an image |\n",
    "| `module\\main.py\\video()`  | 558-562 | replace a frame of a video by another image |\n",
    "| `module\\main.py\\tracker_cars()`  | 565-570 | track vehicle in a video |\n",
    "| `module\\hog_parameters.py\\exp_hog_parameters()`  | 19-135 | compute KPI for a various set of HOG parameters |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
